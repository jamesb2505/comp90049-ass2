{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import __version__ as mpl_version\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import __version__ as skl_version, exceptions\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn import metrics, model_selection, feature_extraction, feature_selection\n",
    "from sklearn import naive_bayes, tree, ensemble, neighbors, linear_model, neural_network, svm, dummy\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sys import version as py_version\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "simplefilter(\"ignore\", category=exceptions.ConvergenceWarning)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(f\"python:     {py_version}\")\n",
    "print(f\"pandas:     {pd.__version__}\")\n",
    "print(f\"numpy:      {np.__version__}\")\n",
    "print(f\"matplotlob: {mpl_version}\")\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"keras:      {keras.__version__}\")\n",
    "print(f\"sklearn:    {skl_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "index_col = \"movieId\"\n",
    "X_train = pd.read_csv(\"./data/train_features.tsv\", sep=\"\\t\", \n",
    "                      index_col=index_col, na_filter=False)\n",
    "Y_train = pd.read_csv(\"./data/train_labels.tsv\", sep=\"\\t\", \n",
    "                      index_col=index_col, na_filter=False).values.ravel()\n",
    "X_valid = pd.read_csv(\"./data/valid_features.tsv\", sep=\"\\t\", \n",
    "                      index_col=index_col, na_filter=False)\n",
    "Y_valid = pd.read_csv(\"./data/valid_labels.tsv\", sep=\"\\t\", \n",
    "                      index_col=index_col, na_filter=False).values.ravel()\n",
    "X_test = pd.read_csv(\"./data/test_features.tsv\", sep=\"\\t\", \n",
    "                     index_col=index_col, na_filter=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic property extraction\n",
    "cols = X_train.columns.values.tolist()\n",
    "avf_cols = [ col for col in cols if \"avf\" in col ]\n",
    "ivec_cols = [ col for col in cols if \"ivec\" in col ]\n",
    "avf_ivec_cols = avf_cols + ivec_cols\n",
    "\n",
    "titles = Counter(word.lower() \n",
    "                 for title in X_train[\"title\"]\n",
    "                 for word in re.split(\"\\W+\", re.sub(\"([^\\W\\s])\", \"\\\\1\", title))\n",
    "                 if word and word not in set(nltk.corpus.stopwords.words(\"english\")))\n",
    "titles = set(title for title, _ in titles.most_common(100))\n",
    "\n",
    "tags = set(tag.lower()\n",
    "           for taglist in X_train[\"tag\"]\n",
    "           for tag in re.sub(\"[,_]\", \" \", taglist).split())\n",
    "\n",
    "genres = list(set(Y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "split_comma = lambda x: x.split(\",\")\n",
    "\n",
    "tf_title = feature_extraction.text.TfidfVectorizer(\n",
    "    strip_accents=\"ascii\"\n",
    ").fit(X_train[\"title\"].apply(lambda x: x.replace(\",\", \" \")))\n",
    "def get_titles(df, tf=tf_title):\n",
    "    return pd.DataFrame(tf.transform(df[\"title\"].apply(lambda x: x.replace(\",\", \" \"))).toarray(), \n",
    "                        columns=[f\"title_{f}\" for f in tf.get_feature_names()],\n",
    "                        index=df.index)\n",
    "\n",
    "tf_tag = feature_extraction.text.TfidfVectorizer(\n",
    "    strip_accents=\"ascii\"\n",
    ").fit(X_train[\"tag\"].apply(lambda x: re.sub(\"[,_]\", \" \", x)))\n",
    "def get_tags(df, tf=tf_tag):\n",
    "    return pd.DataFrame(tf.transform(df[\"tag\"].apply(lambda x: re.sub(\"[,_]\", \" \", x))).toarray(), \n",
    "                        columns=[f\"tag_{f}\" for f in tf.get_feature_names()],\n",
    "                        index=df.index)\n",
    "\n",
    "sc_mm = preprocessing.MinMaxScaler().fit(X_train[avf_ivec_cols])\n",
    "sc_s = preprocessing.StandardScaler().fit(X_train[avf_ivec_cols])\n",
    "def get_ivec_avf(df, sc=sc_mm):\n",
    "    return pd.DataFrame(sc.transform(df[avf_ivec_cols]), \n",
    "                        columns=avf_ivec_cols, \n",
    "                        index=df.index)\n",
    "\n",
    "def get_one_hot(data, label, features):\n",
    "    df = pd.DataFrame(index=data.index)\n",
    "    for feature in features:\n",
    "        df[f\"{label}_{feature}\"] = df.apply(lambda x: int(feature in x), axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess(data):\n",
    "    return [\n",
    "        get_ivec_avf(X).join([\n",
    "            get_tags(X)\n",
    "#           get_titles(X)\n",
    "#           get_one_hot(X[\"tag\"].apply(split_comma), \"tag\", tags)\n",
    "#           get_one_hot(X[\"title\"].apply(split_comma), \"title\", titles)\n",
    "        ]) for X in data\n",
    "    ]\n",
    "\n",
    "X_tr, X_va, X_te = preprocess([X_train, X_valid, X_test])\n",
    "\n",
    "X_size = X_tr.shape[1]\n",
    "Y_size = len(genres)\n",
    "\n",
    "Y_tr = keras.utils.to_categorical(np.array([genres.index(x) for x in Y_train]))\n",
    "Y_va = keras.utils.to_categorical(np.array([genres.index(x) for x in Y_valid]))\n",
    "\n",
    "# joint train and valid data sets\n",
    "X = pd.concat([X_tr, X_va])\n",
    "Y = np.concatenate([Y_train, Y_valid])\n",
    "Y_ = np.concatenate([Y_tr, Y_va])\n",
    "# cross vlaidation split if the use of X, Y/Y_ is required (GridSearchCV)\n",
    "cv = model_selection.PredefinedSplit([-1 if x in X_tr.index else 0 for x in X.index])\n",
    "\n",
    "_ = X_tr.info(), X_va.info(), X_te.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model performance\n",
    "zero_rule = dummy.DummyClassifier(strategy='most_frequent')\n",
    "one_rule = tree.DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "for model, name in [(zero_rule, \"zero rule\"), (one_rule, \"one_rule\")]:\n",
    "    model.fit(X_tr, Y_train)\n",
    "    print(name)\n",
    "    print(metrics.classification_report(Y_valid, model.predict(X_va), zero_division=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model playground\n",
    "\n",
    "# model = tree.DecisionTreeClassifier()\n",
    "# model = ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n",
    "# model = naive_bayes.MultinomialNB()\n",
    "# model = linear_model.Perceptron()\n",
    "# model = svm.SVC(\n",
    "#     C=10,\n",
    "#     kernel=\"poly\", \n",
    "#     degree=3,\n",
    "#     gamma=0.05\n",
    "# )\n",
    "model = neural_network.MLPClassifier(max_iter=1500,\n",
    "                                     hidden_layer_sizes=(80,),\n",
    "                                     solver=\"sgd\")\n",
    "model.fit(X_tr, Y_train)\n",
    "print(model)\n",
    "\n",
    "pred = model.predict(X_va)\n",
    "print(metrics.classification_report(Y_valid, pred, zero_division=0))\n",
    "\n",
    "pred = pd.DataFrame(\n",
    "    model.predict(X_te), \n",
    "    index=X_te.index, \n",
    "    columns=[\"genres\"]\n",
    ")\n",
    "with open(\"Y_test.csv\", \"w\") as f:\n",
    "    f.write(pred.to_csv().replace(\"\\r\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "def fit_grid(model, params, X=X, Y=Y, cv=cv):\n",
    "    grid = model_selection.GridSearchCV(\n",
    "        estimator=model, \n",
    "        param_grid=params, \n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return grid.fit(X, Y)\n",
    "\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [\n",
    "        (n,) * i for i in range(1, 6) for n in range(20, 101, 20)\n",
    "    ],\n",
    "    \"solver\": [\"adam\", \"sgd\", \"lbfgs\"],\n",
    "    \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "    \"alpha\": [ 0.1 ** i for i in range(1, 6) ],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"max_iter\": list(range(100, 3001, 100))\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    \"C\": [ 10 ** i for i in range(3, -4, -1)],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    \"degree\": list(range(2, 9)),\n",
    "    \"gamma\": [\"auto\", \"scale\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data for below\n",
    "result_ = fit_grid(\n",
    "    neural_network.MLPClassifier(max_iter=1500, solver=\"sgd\"), \n",
    "    {k: mlp_params[k] for k in [\"hidden_layer_sizes\"]}\n",
    ")\n",
    "\n",
    "means = result_.cv_results_[\"mean_test_score\"]\n",
    "params = result_.cv_results_[\"params\"]\n",
    "print(f\"Best: {result_.best_score_:.4} {result_.best_params_}\")\n",
    "for mean, param in zip(means, params):\n",
    "    print(f\"{mean:.4} - {param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node configuration heat map\n",
    "arr = [means[i*5:(i+1)*5] for i in range(len(means) // 5)]\n",
    "_, ax = plt.subplots()\n",
    "plt.imshow(arr, cmap=\"viridis\")\n",
    "ax.set_xticklabels([i for i in range(0, 101, 20)])\n",
    "ax.set_yticklabels([str(i) for i in range(0, 6)])\n",
    "plt.xlabel(\"Nodes per Layer\")\n",
    "plt.ylabel(\"Layers\")\n",
    "plt.colorbar()\n",
    "plt.savefig(\"./figs/mlp_config_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_ = fit_grid(\n",
    "    neural_network.MLPClassifier(max_iter=2000), \n",
    "    mlp_params\n",
    ")\n",
    "\n",
    "means = result_.cv_results_[\"mean_test_score\"]\n",
    "params = result_.cv_results_[\"params\"]\n",
    "print(f\"Best: {result_.best_score_:.4} {result_.best_params_}\")\n",
    "for mean, param in zip(means, params):\n",
    "    print(f\"{mean:.4} - {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result__ = fit_grid(\n",
    "    svm.SVC(kernel=\"poly\", C=10),\n",
    "    svc_params\n",
    ")\n",
    "\n",
    "deg_means = result__.cv_results_[\"mean_test_score\"]\n",
    "params = result__.cv_results_[\"params\"]\n",
    "print(f\"Best: {result__.best_score_:.4} {result__.best_params_}\")\n",
    "for mean, param in zip(deg_means, params):\n",
    "    print(f\"{mean:.4} - {param}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def iter_vals(X_tr, Y_tr, X_va, Y_va, models, n_iters, \n",
    "              metric=metrics.accuracy_score, \n",
    "              loss=metrics.log_loss):\n",
    "    iters = list(range(1, n_iters + 1))\n",
    "    vals = [[[], [], [], []] for _ in models]\n",
    "    for model in models:\n",
    "        model.set_params(max_iter=1)\n",
    "    \n",
    "    for i in iters:\n",
    "        for model, (tr_scores, va_scores, tr_loss, va_loss) in zip(models, vals):\n",
    "            model.fit(X_tr, Y_tr)\n",
    "            tr_scores.append(metric(Y_tr, model.predict(X_tr)))\n",
    "            va_scores.append(metric(Y_va, model.predict(X_va)))\n",
    "            tr_loss.append(loss(Y_tr, model.predict_proba(X_tr)))\n",
    "            va_loss.append(loss(Y_va, model.predict_proba(X_va)))\n",
    "    \n",
    "    return iters, vals\n",
    "\n",
    "iters_sgd, [vals_sgd] = iter_vals(X_tr, Y_train, X_va, Y_valid, \n",
    "                                  [neural_network.MLPClassifier(max_iter=1, warm_start=True,\n",
    "                                                                solver=\"sgd\",\n",
    "                                                                hidden_layer_sizes=(60,60))], \n",
    "                                   3000)\n",
    "iters_adam, [vals_adam] = iter_vals(X_tr, Y_train, X_va, Y_valid, \n",
    "                                    [neural_network.MLPClassifier(max_iter=1, warm_start=True,\n",
    "                                                                  solver=\"adam\",\n",
    "                                                                  hidden_layer_sizes=(60,60))],\n",
    "                                    800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy and loss plots\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "tr_score, va_score, tr_loss, va_loss = vals_sgd\n",
    "ax[0][0].plot(iters_sgd, tr_score)\n",
    "ax[0][0].plot(iters_sgd, va_score)\n",
    "ax[0][0].set_xlabel(\"Epoch\")\n",
    "ax[0][0].set_ylabel(\"Accuracy\")\n",
    "ax[0][1].plot(iters_sgd, tr_loss)\n",
    "ax[0][1].plot(iters_sgd, va_loss)\n",
    "ax[0][1].set_yticks([1.0, 1.5, 2.0, 2.5])\n",
    "ax[0][1].set_xlabel(\"Epoch\")\n",
    "ax[0][1].set_ylabel(\"Loss\")\n",
    "\n",
    "tr_score, va_score, tr_loss, va_loss = vals_adam\n",
    "ax[1][0].plot(iters[:300], tr_score[:300])\n",
    "ax[1][0].plot(iters[:300], va_score[:300])\n",
    "ax[1][0].set_xlabel(\"Epoch\")\n",
    "ax[1][0].set_ylabel(\"Accuracy\")\n",
    "ax[1][1].plot(iters[:300], tr_loss[:300])\n",
    "ax[1][1].plot(iters[:300], va_loss[:300])\n",
    "ax[1][1].set_xlabel(\"Epoch\")\n",
    "ax[1][1].set_ylabel(\"Loss\")\n",
    "\n",
    "fig.legend([\"Training Set\", \"Validation Set\"])\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"./figs/loss_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for below\n",
    "result__ = fit_grid(\n",
    "    svm.SVC(kernel=\"poly\", degree=3),\n",
    "    {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    ")\n",
    "\n",
    "C_means = result__.cv_results_[\"mean_test_score\"]\n",
    "C_params = result__.cv_results_[\"params\"]\n",
    "print(f\"Best: {result__.best_score_:.4} {result__.best_params_}\")\n",
    "    \n",
    "result__ = fit_grid(\n",
    "    svm.SVC(kernel=\"poly\", C=10),\n",
    "    {\"degree\": list(range(1, 10))}\n",
    ")\n",
    "\n",
    "deg_means = result__.cv_results_[\"mean_test_score\"]\n",
    "deg_params = result__.cv_results_[\"params\"]\n",
    "print(f\"Best: {result__.best_score_:.4} {result__.best_params_}\")\n",
    "    \n",
    "result__ = fit_grid(\n",
    "    svm.SVC(degree=3, C=10),\n",
    "    {\"kernel\": [\"poly\", \"sigmoid\", \"rbf\"]}\n",
    ")\n",
    "\n",
    "ker_means = result__.cv_results_[\"mean_test_score\"]\n",
    "ker_params = result__.cv_results_[\"params\"]\n",
    "print(f\"Best: {result__.best_score_:.4} {result__.best_params_}\")\n",
    "    \n",
    "    \n",
    "result__ = fit_grid(\n",
    "    svm.SVC(degree=3, C=10, kernel=\"poly\"),\n",
    "    {\"gamma\": [\"scale\", \"auto\"]}\n",
    ")\n",
    "\n",
    "gam_means = result__.cv_results_[\"mean_test_score\"]\n",
    "gam_params = result__.cv_results_[\"params\"]\n",
    "print(f\"Best: {result__.best_score_:.4} {result__.best_params_}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm param plots\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "def get_labels(params):\n",
    "    return [str(list(x.values())[0]) for x in params]\n",
    "\n",
    "ax[0][0].bar(get_labels(C_params), C_means)\n",
    "ax[0][0].set_title(\"C\")\n",
    "ax[0][1].bar(get_labels(deg_params), deg_means)\n",
    "ax[0][1].set_title(\"Degree\")\n",
    "ax[1][0].bar(get_labels(ker_params), ker_means)\n",
    "ax[1][0].set_title(\"Kernel\")\n",
    "ax[1][1].bar([\"scaled\", \"unscaled\"], gam_means)\n",
    "ax[1][1].set_title(u\"\\u03b3\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"./figs/svm_bars.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toying with keras\n",
    "# kept for reference only\n",
    "\n",
    "# # 0.37 \n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Dense(80, input_shape=(X_tr.shape[1],), activation=\"relu\"),\n",
    "#     keras.layers.Dense(80, activation=\"relu\"),\n",
    "#     keras.layers.Dense(len(genres), activation=\"softmax\")\n",
    "# ])\n",
    "\n",
    "# # 0.37\n",
    "# # model = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(X_tr.shape[1] - (X_tr.shape[1] + len(genres)) // 4, input_shape=(X_tr.shape[1],), activation=\"relu\"),\n",
    "# #     keras.layers.Dense((X_tr.shape[1] + len(genres)) // 2, activation=\"relu\"),\n",
    "# #     keras.layers.Dense(X_tr.shape[1] - 3 * (X_tr.shape[1] + len(genres)) // 4, activation=\"relu\"),\n",
    "# #     keras.layers.Dense(len(genres), activation=\"softmax\")\n",
    "# # ])\n",
    "\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy', \n",
    "#     optimizer=\"sgd\", \n",
    "#     metrics=[\n",
    "#         \"accuracy\", \n",
    "#         keras.metrics.Precision(name=\"precision\"),\n",
    "#         keras.metrics.Recall(name=\"recall\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# history = model.fit(X_tr, Y_tr, epochs=500, batch_size=64, validation_data=(X_va, Y_va), verbose=False)\n",
    "\n",
    "# pred = pd.DataFrame(\n",
    "#     np.array([genres[np.argmax(x)] for x in model.predict(X_te)]), \n",
    "#     index=X_te.index, \n",
    "#     columns=[\"genres\"]\n",
    "# )\n",
    "# with open(\"Y_test.csv\", \"w\") as f:\n",
    "#     f.write(pred.to_csv().replace(\"\\r\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more keras playing\n",
    "# kept for reference only\n",
    "\n",
    "# def build_model(layers=(60,40), \n",
    "#                 activation=\"relu\", \n",
    "#                 batch_size=64, \n",
    "#                 optimizer=\"adam\",\n",
    "#                 learning_rate=0.01,\n",
    "#                 dropout=None):\n",
    "    \n",
    "#     model = keras.models.Sequential()\n",
    "    \n",
    "#     model.add(keras.layers.Dense(layers[0], input_shape=(X_size,), activation=activation))\n",
    "#     for layer in layers[1:]:\n",
    "#         if dropout is not None:\n",
    "#             model.add(keras.layers.Dropout(dropout))\n",
    "#         model.add(keras.layers.Dense(layer, activation=activation))\n",
    "#     if dropout is not None:\n",
    "#         model.add(keras.layers.Dropout(dropout))\n",
    "#     model.add(keras.layers.Dense(Y_size, activation=\"softmax\"))\n",
    "    \n",
    "#     if optimizer == \"sgd\":\n",
    "#         optimizer = keras.optimizers.SGD\n",
    "#     else: \n",
    "#         optimizer = keras.optimizers.Adam\n",
    "#     optimizer = optimizer(learning_rate=learning_rate)\n",
    "    \n",
    "#     model.compile(\n",
    "#         loss=\"categorical_crossentropy\", \n",
    "#         optimizer=optimizer, \n",
    "#         metrics=[\n",
    "#             \"accuracy\", \n",
    "#             keras.metrics.Precision(name=\"precision\"),\n",
    "#             keras.metrics.Recall(name=\"recall\")\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, verbose=False)\n",
    "\n",
    "# layers = [\n",
    "#     (100, 100, 100, 100), (80, 80, 80, 80), (60, 60, 60, 60), (40, 40, 40, 40),\n",
    "#     (100, 100, 100), (80, 80, 80), (60, 60, 60), (40, 40, 40),\n",
    "#     (100, 100), (80, 80), (60, 60), (40, 40),\n",
    "#     (100,), (80,), (60,), (40,),\n",
    "#     (100, 80, 60, 40), (100, 75, 50), (100, 50)\n",
    "# ]\n",
    "# grid_params = (\n",
    "#     (\"layers\", layers),\n",
    "#     (\"activation\", [\"relu\", \"sigmoid\", \"tanh\"]),\n",
    "#     (\"batch_size\", [ 2 ** i for i in range(11) ]),\n",
    "#     (\"optimizer\", [\"adam\", \"sgd\"]),\n",
    "#     (\"learning_rate\", [ 0.1 ** i for i in range(1, 5) ]),\n",
    "#     (\"dropout\", [ 0.1 * i for i in range(1, 10) ]),\n",
    "#     (\"epochs\", [ i for i in range(25, 50, 5) ])\n",
    "# )\n",
    "# grid_params = { p: (p, r) for p, r in grid_params }\n",
    "\n",
    "# grid = dict([\n",
    "#     grid_params[\"epochs\"]\n",
    "# ])\n",
    "\n",
    "# grid_cv = model_selection.GridSearchCV(\n",
    "#     estimator=model, \n",
    "#     param_grid=grid, \n",
    "#     cv=cv,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# result = grid_cv.fit(X, Y_)\n",
    "\n",
    "# means = result.cv_results_[\"mean_test_score\"]\n",
    "# params = result.cv_results_[\"params\"]\n",
    "# print(f\"Test: {list(grid.keys())}\")\n",
    "# print(f\"Best: {result.best_score_:.4} {result.best_params_}\")\n",
    "# for mean, param in zip(means, params):\n",
    "#     print(f\"{mean:.4} - {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
